<p align="center">
  <a href="#Graph-MLLM">Overview</a> •
  <a href="#installation">Installation</a> •
  <a href="">Examples</a> •
  <a href="">Docs</a>
</p>

# Graph-MLLM

Official code for ***Graph**-**MLLM**: Harnessing Multimodal Large Language Models for Multimodal **Graph** Learning*. Graph-MLLM is a comprehensive benchmark for multimodal graph learning that extends state-of-the-art graph methods into the multimodal domain using MLLMs. Fusing multimodal data with graph-structured data shows great promise for numerous real-world applications—such as social networks, healthcare, and recommendation systems—when nodes contain both textual and visual attributes.


## Overview of Graph-MLLM

Graph-MLLM provides a fair and comprehensive platform for evaluating existing graph learning methods and facilitating future research on multimodal graph learning.

![graphmllm](https://github.com/oamyjin/Graph-MLLM/blob/main/docs/graphmllm.png)




huggingface: https://huggingface.co/datasets/oamyjin/Graph-MLLM


augmentor: base_model: vicuna-7b-v1.5-16k:https://huggingface.co/lmsys/vicuna-7b-v1.5-16k/tree/main
